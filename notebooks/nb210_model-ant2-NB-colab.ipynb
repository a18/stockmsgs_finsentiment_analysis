{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddc1c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version history:\n",
    "# 2022-12-06: created from nb210_model-ant2-NB-colab.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "McUNYSRES3BJ",
   "metadata": {
    "id": "McUNYSRES3BJ"
   },
   "source": [
    "# ===== Part0 - env preparation ====="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde3a800",
   "metadata": {
    "id": "fde3a800"
   },
   "source": [
    "## System info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c83417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print system id\n",
    "!nvidia-smi\n",
    "!hostname\n",
    "!uname -a\n",
    "!df -kh /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -V  # If version < 3.9 then some f-string features may not work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f654058b",
   "metadata": {
    "id": "f654058b"
   },
   "source": [
    "## Mount drive (if required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf93c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TO_USE_COLAB = None\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    TO_USE_COLAB = True\n",
    "except:\n",
    "    TO_USE_COLAB = False\n",
    "TO_USE_COLAB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xupoqKnA_HuV",
   "metadata": {
    "id": "xupoqKnA_HuV"
   },
   "source": [
    "## Env vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NEihSnNk-_sf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # NEW 2022-12-05, see https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb56906c",
   "metadata": {
    "id": "eb56906c"
   },
   "source": [
    "# ===== Part 1: prepare dataset ====="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbb4236",
   "metadata": {
    "id": "fcbb4236"
   },
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b075d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea14453e",
   "metadata": {
    "id": "ea14453e"
   },
   "source": [
    "## Paths and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000879da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ts():\n",
    "  return datetime.datetime.now(tz=pytz.timezone(\"Europe/Minsk\")).strftime(\"%Y-%m-%dT%H%M%S\")\n",
    "START_TS = get_ts()\n",
    "START_TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9edd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TO_USE_COLAB:\n",
    "    PATH_MAIN_DIR = f\"/content/drive/MyDrive/_PR_ROOT/_2022/2022-11_NLP-Huawei_Final_project/stocktwits_finsentiment_analysis/notebooks\"\n",
    "else:\n",
    "    PATH_MAIN_DIR = \".\"\n",
    "assert os.path.isdir(PATH_MAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f8f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $PATH_MAIN_DIR\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbf2224",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_OUT_DIR = f\"../data/interim/050_output__nb200/_out_dir_{START_TS}\"\n",
    "os.mkdir(PATH_OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8566f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files and folders\n",
    "\n",
    "DIR_DATA_SRC = r'../data/interim/040_output__nb010_v1'\n",
    "#FNAMES = ['VIX_RmSW=0_RmRep=0_1y_top10.csv', 'VIX_RmSW=0_RmRep=0_1y_top10.csv' ]  # Loads in <1 sec\n",
    "FNAMES = ['AMZN_RmSW=0_RmRep=0_1y.csv.gz', 'NFLX_RmSW=0_RmRep=0_1y.csv.gz', ]  # Loads in <1 sec\n",
    "#FNAMES = ['AAPL_RmSW=0_RmRep=0_1y.csv.gz', ]  # Loads in 20-30 sec\n",
    "\n",
    "assert os.path.isdir(DIR_DATA_SRC)\n",
    "for f in FNAMES:\n",
    "    assert os.path.isfile(os.path.join(DIR_DATA_SRC, f)), f\"File not found: {f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5806080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset preparation settings\n",
    "\n",
    "DROP_RECORDS_BEFORE_DATE_INCLUSIVE = '2019-07-20'  # Last date in datasets is 2020-07-21\n",
    "LABEL_GEN_STRATEGY = \"d1_C=d1_O=0.5%=2cls\"  # This string is a \"key\", see function XXX for explanations\n",
    "COL_FEATURES = ['symbol', 'message', 'datetime', 'user', 'message_id', 'Date']  #, 'Time']\n",
    "COL_LABEL = 'label'\n",
    "COL_PCR = 'price_change_ratio'\n",
    "\n",
    "# SPLIT_SHUFFLING_SEED = 42  # If None, then no shuffling is done\n",
    "TEST_SIZE = 0.15\n",
    "TRAIN_SIZE = 1.0 - TEST_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca878418",
   "metadata": {
    "id": "ca878418"
   },
   "source": [
    "## Defs\n",
    "Here are \"pure\" functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8863e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_df_details(df: pd.DataFrame):\n",
    "    print(\"\\nHead:\\n\", df.head())\n",
    "    print(\"\\nTail:\\n\", df.tail())\n",
    "    print('\\nInfo:')\n",
    "    df.info()  # This method prints by itself\n",
    "    print('\\nDescribe:\\n', df.describe(include='all'))  #, datetime_is_numeric=True)) - to suppress warnings   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eea0a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pandas_file(file_path: str, verbose=True):\n",
    "    # Prepare\n",
    "    assert os.path.isfile(file_path), f\"Cannot find file: '{file_path}', cur folder: '{os. getcwd()}'\"    \n",
    "    print(\"Loading data from: \", file_path)\n",
    "        \n",
    "    # Do the load\n",
    "    start_time = time.time()\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Success. Shape: {df.shape}, elapsed seconds: {time.time() - start_time:.2f}\")\n",
    "    \n",
    "    # Dump details if required\n",
    "    if verbose:\n",
    "        print_df_details(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406b6d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dfs(df_list: list, verbose=True) -> pd.DataFrame:\n",
    "    if verbose:\n",
    "        for df in df_list:\n",
    "            print(df.shape, end=';')\n",
    "    res_df = pd.concat(df_list, ignore_index=True)\n",
    "    if verbose:\n",
    "        print(\"->\", res_df.shape)\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906f0941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ts():\n",
    "  return datetime.datetime.now(tz=pytz.timezone(\"Europe/Minsk\")).strftime(\"%Y-%m-%dT%H%M%S\")\n",
    "START_TS = get_ts()\n",
    "START_TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0bc300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_old_dates_inplace(df: pd.DataFrame, drop_date_inclusive: str, verbose=True) -> pd.DataFrame:\n",
    "    assert isinstance(drop_date_inclusive, str)\n",
    "    old_shape = df.shape\n",
    "    df.drop(df[df['Date'] <= drop_date_inclusive].index, inplace = True)\n",
    "    print(f\"Old dates dropped. Shape before: {old_shape}, after: {df.shape}\")\n",
    "    if verbose:\n",
    "        print_df_details(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0611db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(ch):\n",
    "  if ch > 0.5:\n",
    "    return 1\n",
    "  elif ch < -0.5:\n",
    "    return -1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "\n",
    "def generate_labels_and_pcr_list(df: pd.DataFrame, strategy_str: str) -> list:\n",
    "    # price_change_ratio = pcr \n",
    "    if strategy_str == \"d1_C=d1_O=0.5%=2cls\":\n",
    "        assert (df['d1_O'] > 0.0).all()  # Prices must be > 0\n",
    "        assert (df['d1_C'] > 0.0).all()  # Prices must be > 0\n",
    "        rel_change_perc = (df['d1_C'] / df['d1_O'] - 1.0) * 100.0\n",
    "        # Convert from percentages to labels -1, 0, 1\n",
    "        res_series = rel_change_perc.apply(get_label)\n",
    "    else:\n",
    "        assert False, \"Unexpeced strategy_str\"\n",
    "    return res_series.to_list(), rel_change_perc.to_list()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b3a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_feature_selection(df: pd.DataFrame):\n",
    "    res_df = df[COL_FEATURES]\n",
    "    print(f\"Selected cols: {res_df.columns}\")\n",
    "    return res_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893d9ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_label_transformation(df: pd.DataFrame):\n",
    "    temp_df = df.drop(df[df[COL_LABEL] == 0].index, inplace= False).copy()\n",
    "    temp_df[COL_LABEL].replace({-1:0}, inplace = True)\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d63c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_real_profit_perc(y_pred, pcr_list) -> float:\n",
    "    return np.NaN  # TODO: This function is not correct, as it's necessary to aggregate predictions by date and ticker\n",
    "\n",
    "    profit_ratio = 1.0\n",
    "    assert len(y_pred) == len(pcr_list), f\"{len(y_pred)}, {len(pcr_list)}\"\n",
    "    for i, (pred, pcr) in enumerate(zip(y_pred, pcr_list)):\n",
    "        price_ratio = (pcr / 100.0 + 1.0)  # Convert from percents [-5% .. 5%] -> [-0.05 .. 0.05] -> [0.95 .. 1.05]\n",
    "        assert 0.0 < price_ratio < np.inf, f\"{i}, {price_ratio}\" \n",
    "        if pred == 1:\n",
    "            # Long\n",
    "            profit_ratio *= price_ratio\n",
    "        elif pred == 0:\n",
    "            # Short\n",
    "            profit_ratio /= price_ratio\n",
    "        else:\n",
    "            assert False, \"Unexpected label\"\n",
    "    return (profit_ratio - 1.0) * 100.0  # Profit in percents (0% - nothing changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ef9cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hash_for_seq(values, hash_len=6):\n",
    "    assert isinstance(values, (list, np.ndarray, pd.Series))\n",
    "    h = hash(tuple(values))\n",
    "    return str(h)[-hash_len:]\n",
    "\n",
    "# Small unit tests\n",
    "print(calc_hash_for_seq([1, 2, 3]))\n",
    "print(calc_hash_for_seq(np.array([1, 2, 3])))\n",
    "print(calc_hash_for_seq(pd.Series([1, 2, 3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TdLOTzd9v63y",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_label_distribution_equal(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    counts = df.label.value_counts()\n",
    "    assert len(counts == 2)  # We expect only labels 0 and 1\n",
    "\n",
    "    bigger_label = 0 if counts[0] > counts[1] else 1\n",
    "    diff = abs(counts[0] - counts[1])\n",
    "\n",
    "    res_df = df.drop(index=df[df.label == bigger_label].sample(n = diff, replace=False, random_state=42).index)\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b61a358",
   "metadata": {
    "id": "2b61a358"
   },
   "source": [
    "## Do prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf62af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data, dropping old dates\n",
    "df_list = []\n",
    "for fname in FNAMES:\n",
    "    full_name = os.path.join(DIR_DATA_SRC, fname)\n",
    "    assert os.path.isfile(full_name), full_name\n",
    "    df_temp = load_pandas_file(full_name, verbose=False)\n",
    "    drop_old_dates_inplace(df_temp, DROP_RECORDS_BEFORE_DATE_INCLUSIVE, verbose=False)\n",
    "    df_list.append(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdf3696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat loaded parts to one dataframe\n",
    "df_raw = merge_dfs(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f2c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose columns for final dataset\n",
    "df_final = do_feature_selection(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8e6928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the target column\n",
    "labels, pcr_list = generate_labels_and_pcr_list(df_raw, strategy_str=LABEL_GEN_STRATEGY)\n",
    "df_final[COL_LABEL] = labels\n",
    "df_final[COL_PCR] = pcr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bac67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop labels for neutral class\n",
    "df_final = do_label_transformation(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb7725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_df_details(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c496fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[COL_LABEL].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EYHENtC2wQ25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making labels distribution equal\n",
    "df_final = make_label_distribution_equal(df_final)\n",
    "df_final[COL_LABEL].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d862a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cad7c0",
   "metadata": {
    "id": "93cad7c0"
   },
   "source": [
    "# ===== Part 2: Model execution and scoring ====="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292b8bad",
   "metadata": {
    "id": "292b8bad"
   },
   "source": [
    "## Imports (part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df081ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if TO_USE_COLAB:\n",
    "#    !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4ee10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import gensim.downloader\n",
    "# import matplotlib.pyplot as plt\n",
    "# from optuna import create_study\n",
    "# from pprint import pprint\n",
    "# import random\n",
    "# from sklearn.dummy import DummyClassifier\n",
    "# from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch.nn.utils.rnn import pack_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T6rV21w-9FvS",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline-related imports\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# NB-related imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e135ce1",
   "metadata": {
    "id": "8e135ce1"
   },
   "source": [
    "## Defs (part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67df3605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_and_get_predictions__sklearn_classifier(model_tag: str, \n",
    "    X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.Series, seed: int) -> pd.Series:\n",
    "\n",
    "    # Initial checks\n",
    "    assert COL_PCR not in X_train.columns  # To avoid data leaks\n",
    "    \n",
    "    # Create the model with specified seed\n",
    "    if model_tag == \"dummy__most_frequent\":\n",
    "        model = DummyClassifier(strategy=\"most_frequent\", random_state=seed)\n",
    "    elif model_tag == \"dummy__uniform\":\n",
    "        model = DummyClassifier(strategy=\"uniform\", random_state=seed)\n",
    "    else:\n",
    "        assert False, f\"Unexpected model tag: {model_tag}\"\n",
    "    \n",
    "    # Train the model    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    return y_pred    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gw7qhP2cL-XI",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_and_get_predictions__NB_classifier(model_tag: str, \n",
    "    X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.Series, seed: int) -> pd.Series:\n",
    "\n",
    "    # Initial checks\n",
    "    assert COL_PCR not in X_train.columns  # To avoid data leaks\n",
    "    \n",
    "    # Create the vectorizer and model with specified params\n",
    "    if model_tag == \"NaiveBayes_words_ng1-3_alhpa0.1\":\n",
    "        tf_idf = TfidfVectorizer(use_idf=True, ngram_range=(1,3), analyzer='word')\n",
    "        model = MultinomialNB(alpha=0.1)\n",
    "    elif model_tag == \"NaiveBayes_words_ng2-2_alhpa0.1\":\n",
    "        tf_idf = TfidfVectorizer(use_idf=True, ngram_range=(2,2), analyzer='word')\n",
    "        model = MultinomialNB(alpha=0.1)        \n",
    "    else:\n",
    "        assert False, f\"Unexpected model tag: {model_tag}\"\n",
    "    \n",
    "    # Prepare tf-idf features (!huge sparse matrix)\n",
    "    train_features = tf_idf.fit_transform(X_train.message)\n",
    "    test_features = tf_idf.transform(X_test.message)\n",
    "\n",
    "    # Train the model    \n",
    "    model.fit(train_features, y_train)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = model.predict(test_features)\n",
    "    \n",
    "    return y_pred    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d8f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_DEBUG_STOP = False  # The code below will be stopped after putting data into global vars\n",
    "\n",
    "# Launch split-train-predict-metrics cycle for several seeds\n",
    "def get_model_score_distribution(model_tag: str, df: pd.DataFrame, launch_cnt: int = 5, verbose=True):\n",
    "\n",
    "    # Global vars, required for DO_DEBUG_STOP case, to continue writing code on the root notebook level\n",
    "    global X_train, y_train, X_test, y_test, seed\n",
    "\n",
    "    result = []\n",
    "    print(\"Legend: seed; X_train shape; X_test_shape; y_train shape,hash,sum; y_test shape,hash,sum\")\n",
    "    for seed in range(42, 42 + launch_cnt):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            df[COL_FEATURES + [COL_PCR]], df[COL_LABEL],\n",
    "            # stratify=df[COL_LABEL], # Note: stratification leads to the same test set (though shuffled)\n",
    "            shuffle=True,\n",
    "            random_state = seed, \n",
    "            test_size = TEST_SIZE\n",
    "        )\n",
    "        # Note: equal hash means binary equality, equal sum means the same rows but shuffled\n",
    "        print(f\"After split: {seed}, {X_train.shape}; {X_test.shape}; {y_train.shape},{calc_hash_for_seq(y_train)},{sum(y_train)};\"\n",
    "              + f\" {y_test.shape},{calc_hash_for_seq(y_test)},{sum(y_test)}\")\n",
    "\n",
    "        # Separate price_change_ratio from the data\n",
    "        pcr_train = X_train[COL_PCR]; X_train.drop(COL_PCR, axis=1, inplace=True)\n",
    "        pcr_test = X_test[COL_PCR]; X_test.drop(COL_PCR, axis=1, inplace=True)\n",
    "\n",
    "        if DO_DEBUG_STOP:\n",
    "            assert False, \"Debug-stop fired. Now you could use the above global vars on any notebook cells.\"\n",
    "\n",
    "        # Launch model-specific method\n",
    "        y_pred = None\n",
    "        if model_tag.startswith('dummy_'):\n",
    "            y_pred = train_model_and_get_predictions__sklearn_classifier(model_tag, X_train, y_train, X_test, seed)\n",
    "        elif model_tag.startswith('NaiveBayes_'):\n",
    "            y_pred = train_model_and_get_predictions__NB_classifier(model_tag, X_train, y_train, X_test, seed)\n",
    "        else:\n",
    "            assert False, f\"Unexpected model tag: {model_tag}\"\n",
    "\n",
    "        # Calc score\n",
    "        score1 = accuracy_score(y_test, y_pred)\n",
    "        #score2 = calc_real_profit_perc(y_pred, pcr_test)\n",
    "        #score3 = calc_real_profit_perc(y_train[:100], pcr_train[:100])\n",
    "        #result.append(f\"{score1:.5f}, {score2:.2f}%, {score3:.2f}%\")\n",
    "        result.append(score1)\n",
    "            \n",
    "        if verbose:\n",
    "            print(confusion_matrix(y_test, y_pred))\n",
    "            print(classification_report(y_test, y_pred, digits=3))\n",
    "                    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801c663a",
   "metadata": {
    "id": "801c663a"
   },
   "source": [
    "## Launch the model training/estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6fce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tag = \"dummy__most_frequent\"\n",
    "results = get_model_score_distribution(model_tag, df_final, verbose=False)\n",
    "print(\"Sorted results (accuracy):\", sorted(results))\n",
    "print(f\"Mean accuracy: {np.mean(results):.3f} +- {np.std(results):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e48423",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tag = \"dummy__uniform\"\n",
    "results = get_model_score_distribution(model_tag, df_final, launch_cnt = 5, verbose=False)\n",
    "print(\"Sorted results (accuracy):\", sorted(results))\n",
    "print(f\"Mean accuracy: {np.mean(results):.3f} +- {np.std(results):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3717bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_tag = \"NaiveBayes_words_ng1-3_alhpa0.1\"  # Mean accuracy: 0.568 +- 0.002\n",
    "model_tag = \"NaiveBayes_words_ng2-2_alhpa0.1\"  # Mean accuracy: 0.559 +- 0.002\n",
    "results = get_model_score_distribution(model_tag, df_final, launch_cnt = 5, verbose=False)\n",
    "print(\"Sorted results (accuracy):\", sorted(results))\n",
    "print(f\"Mean accuracy: {np.mean(results):.3f} +- {np.std(results):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XffGiornJNar",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
